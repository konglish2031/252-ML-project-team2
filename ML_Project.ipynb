{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "832c9856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.35.2.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas import Series\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "\n",
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_theme(font_scale=2.5) \n",
    "# 이 두줄은 본 필자가 항상 쓰는 방법입니다. \n",
    "# matplotlib 의 기본 scheme 말고 seaborn scheme 을 세팅하고, 일일이 graph 의 font size 를 지정할 필요 없이 seaborn 의 font_scale 을 사용하면 편합니다.\n",
    "import missingno as msno\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "4b032fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/titanic/kaggle/input\\gender_submission.csv\n",
      "C:/titanic/kaggle/input\\gender_submission_original.csv\n",
      "C:/titanic/kaggle/input\\test.csv\n",
      "C:/titanic/kaggle/input\\test_original.csv\n",
      "C:/titanic/kaggle/input\\train.csv\n",
      "C:/titanic/kaggle/input\\train_original.csv\n",
      "C:/titanic/kaggle/input\\zNex~$haretest_original.csv\n",
      "C:/titanic/kaggle/input\\zNex~$haretrain.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('C:/titanic/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "0bbb035a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsurvival :  생존여부(target label 임)(1,0 으로 표현됨)    // integer\\nPclass :    티켓의 클래스 1 = 1st, 2 = 2nd, 3 = 3rd     // integer\\nsex :       성별(male, female 로 구분됨)                // string\\nAge :       나이                                      // continuous integer\\nsibSp :     함께 탑승한 형제와 배우자의 수                // quantitative integer\\nparch :     함께 탑승한 부모, 아이의 수                  // quantitative integer\\nticket :    티켓 번호                                  // alphabat + integer string\\nfare :      탑승료                                     // continuous float\\ncabin :     객실 번호                                  // alphabat + integer string\\nembarked :  탑승 항구                                   // C = Cherbourg, Q = Queenstown, S = Southampton string\\n'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('C:/titanic/kaggle/input/train.csv')\n",
    "test = pd.read_csv('C:/titanic/kaggle/input/test.csv')\n",
    "train.head()\n",
    "\n",
    "'''\n",
    "survival :  생존여부(target label 임)(1,0 으로 표현됨)    // integer\n",
    "Pclass :    티켓의 클래스 1 = 1st, 2 = 2nd, 3 = 3rd     // integer\n",
    "sex :       성별(male, female 로 구분됨)                // string\n",
    "Age :       나이                                      // continuous integer\n",
    "sibSp :     함께 탑승한 형제와 배우자의 수                // quantitative integer\n",
    "parch :     함께 탑승한 부모, 아이의 수                  // quantitative integer\n",
    "ticket :    티켓 번호                                  // alphabat + integer string\n",
    "fare :      탑승료                                     // continuous float\n",
    "cabin :     객실 번호                                  // alphabat + integer string\n",
    "embarked :  탑승 항구                                   // C = Cherbourg, Q = Queenstown, S = Southampton string\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "48e64a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss percentage of missing values in train set\n",
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n",
      "\n",
      "loss percentage of missing values in test set\n",
      "PassengerId      0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age             86\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             1\n",
      "Cabin          327\n",
      "Embarked         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"loss percentage of missing values in train set\\n{train.isnull().sum()}\") # loss of missing values in train set\n",
    "print(f\"\\nloss percentage of missing values in test set\\n{test.isnull().sum()}\") # loss of missing values in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "8a87f037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss of missing values in train set\n",
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         0\n",
      "dtype: int64\n",
      "\n",
      "loss of missing values in test set\n",
      "PassengerId      0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age             86\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             1\n",
      "Cabin          327\n",
      "Embarked         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Embarked 결측값 처리: pclass를 참고하여 1이면 C, 2이면 S, 3이면 S로 채우기\n",
    "def fill_missing_embarked(row):\n",
    "    if pd.isnull(row['Embarked']): # Embarked가 결측치인 경우\n",
    "        if row['Pclass'] == 1:\n",
    "            return 'C'\n",
    "        elif row['Pclass'] == 2:\n",
    "            return 'S'\n",
    "        elif row['Pclass'] == 3:\n",
    "            return 'S'\n",
    "    return row['Embarked']\n",
    "\n",
    "train['Embarked'] = train.apply(fill_missing_embarked, axis=1)\n",
    "\n",
    "print(f\"loss of missing values in train set\\n{train.isnull().sum()}\") # loss of missing values in train set\n",
    "print(f\"\\nloss of missing values in test set\\n{test.isnull().sum()}\") # loss of missing values in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "8f73171a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median fare for Pclass 3 and Embarked S: 8.05\n"
     ]
    }
   ],
   "source": [
    "# test 의 fare의 결측치 1개 : pclass 3, embarked S, age 60.5\n",
    "# test 의 fare의 결측치 처리: pclass 3, embarked S 인 승객들의 fare의 중앙값으로 채우기\n",
    "median_fare = test[(test['Pclass'] == 3) & (test['Embarked'] == 'S')]['Fare'].median()\n",
    "print(f\"Median fare for Pclass 3 and Embarked S: {median_fare}\")\n",
    "test['Fare'].fillna(median_fare, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "18991c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss of missing values in train set\n",
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         0\n",
      "dtype: int64\n",
      "\n",
      "loss of missing values in test set\n",
      "PassengerId      0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age             86\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          327\n",
      "Embarked         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"loss of missing values in train set\\n{train.isnull().sum()}\") # loss of missing values in train set\n",
    "print(f\"\\nloss of missing values in test set\\n{test.isnull().sum()}\") # loss of missing values in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92683ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Sex_Encoded</th>\n",
       "      <th>Embarked_Encoded</th>\n",
       "      <th>Title_Encoded</th>\n",
       "      <th>family_size</th>\n",
       "      <th>is_alone</th>\n",
       "      <th>Fare_log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1309.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1046.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>655.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.294882</td>\n",
       "      <td>29.881138</td>\n",
       "      <td>0.498854</td>\n",
       "      <td>0.385027</td>\n",
       "      <td>33.276193</td>\n",
       "      <td>0.644003</td>\n",
       "      <td>1.490451</td>\n",
       "      <td>1.900688</td>\n",
       "      <td>1.883881</td>\n",
       "      <td>0.603514</td>\n",
       "      <td>2.978817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>378.020061</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.837836</td>\n",
       "      <td>14.413493</td>\n",
       "      <td>1.041658</td>\n",
       "      <td>0.865560</td>\n",
       "      <td>51.743584</td>\n",
       "      <td>0.478997</td>\n",
       "      <td>0.816089</td>\n",
       "      <td>0.786492</td>\n",
       "      <td>1.583639</td>\n",
       "      <td>0.489354</td>\n",
       "      <td>0.968536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>328.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.895800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.185579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>655.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.737881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>982.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.275000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.474293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>512.329200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.240917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived       Pclass          Age        SibSp  \\\n",
       "count  1309.000000  891.000000  1309.000000  1046.000000  1309.000000   \n",
       "mean    655.000000    0.383838     2.294882    29.881138     0.498854   \n",
       "std     378.020061    0.486592     0.837836    14.413493     1.041658   \n",
       "min       1.000000    0.000000     1.000000     0.170000     0.000000   \n",
       "25%     328.000000    0.000000     2.000000    21.000000     0.000000   \n",
       "50%     655.000000    0.000000     3.000000    28.000000     0.000000   \n",
       "75%     982.000000    1.000000     3.000000    39.000000     1.000000   \n",
       "max    1309.000000    1.000000     3.000000    80.000000     8.000000   \n",
       "\n",
       "             Parch         Fare  Sex_Encoded  Embarked_Encoded  Title_Encoded  \\\n",
       "count  1309.000000  1309.000000  1309.000000       1309.000000    1309.000000   \n",
       "mean      0.385027    33.276193     0.644003          1.490451       1.900688   \n",
       "std       0.865560    51.743584     0.478997          0.816089       0.786492   \n",
       "min       0.000000     0.000000     0.000000          0.000000       0.000000   \n",
       "25%       0.000000     7.895800     0.000000          1.000000       2.000000   \n",
       "50%       0.000000    14.454200     1.000000          2.000000       2.000000   \n",
       "75%       0.000000    31.275000     1.000000          2.000000       2.000000   \n",
       "max       9.000000   512.329200     1.000000          2.000000       4.000000   \n",
       "\n",
       "       family_size     is_alone     Fare_log  \n",
       "count  1309.000000  1309.000000  1309.000000  \n",
       "mean      1.883881     0.603514     2.978817  \n",
       "std       1.583639     0.489354     0.968536  \n",
       "min       1.000000     0.000000     0.000000  \n",
       "25%       1.000000     0.000000     2.185579  \n",
       "50%       1.000000     1.000000     2.737881  \n",
       "75%       2.000000     1.000000     3.474293  \n",
       "max      11.000000     1.000000     6.240917  "
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Egineering\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 러닝에 사용되는 feature 선정: name_title, sex, embarked, pclass, family size, fare\n",
    "#    - Name에서 Title 추출\n",
    "train['Title'] = train['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "test['Title'] = test['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "\n",
    "#      일반적이지 않은 호칭들을 'Rare'로 그룹화\n",
    "rare_titles = ['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona']\n",
    "train['Title'] = train['Title'].replace(rare_titles, 'Rare')\n",
    "test['Title'] = test['Title'].replace(rare_titles, 'Rare')\n",
    "\n",
    "#       Mlle, Ms, Mme 같은 유사 호칭 통합\n",
    "train['Title'] = train['Title'].replace('Mlle', 'Miss')\n",
    "train['Title'] = train['Title'].replace('Ms', 'Miss')\n",
    "train['Title'] = train['Title'].replace('Mme', 'Mrs')\n",
    "test['Title'] = test['Title'].replace('Mlle', 'Miss')\n",
    "test['Title'] = test['Title'].replace('Ms', 'Miss')\n",
    "test['Title'] = test['Title'].replace('Mme', 'Mrs')\n",
    "\n",
    "#    - Sex, Embarked, Pclass 수치화 (LabelEncoder 사용)\n",
    "#a.인코더 객체를 변수별로 생성합니다.\n",
    "le_sex = LabelEncoder()\n",
    "le_embarked = LabelEncoder()\n",
    "le_title = LabelEncoder()\n",
    "\n",
    "#b.'train' 데이터로 'fit'과 'transform'을 동시에 수행합니다.\n",
    "train['Sex_Encoded'] = le_sex.fit_transform(train['Sex'])\n",
    "train['Embarked_Encoded'] = le_embarked.fit_transform(train['Embarked'])\n",
    "train['Title_Encoded'] = le_title.fit_transform(train['Title'])\n",
    "\n",
    "#c.'test' 데이터는 'transform'만 수행합니다. (train 기준으로 변환)\n",
    "#    (주의: test에 train에 없던 새로운 값이 있으면 에러가 발생할 수 있습니다.)\n",
    "test['Sex_Encoded'] = le_sex.transform(test['Sex'])\n",
    "test['Embarked_Encoded'] = le_embarked.transform(test['Embarked'])\n",
    "test['Title_Encoded'] = le_title.transform(test['Title'])\n",
    "\n",
    "#    - family size (SibSp + Parch) 추가 고려 가능\n",
    "train['family_size'] = train.SibSp + train.Parch + 1\n",
    "train['is_alone'] = train.family_size.apply(lambda x: 1 if x == 1 else 0)\n",
    "test['family_size'] = test.SibSp + test.Parch + 1\n",
    "test['is_alone'] = test.family_size.apply(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "#   - fare의 분포를 균등하게 만들기\n",
    "train['Fare_log'] = np.log1p(train['Fare'])\n",
    "test['Fare_log'] = np.log1p(test['Fare'])\n",
    "\n",
    "\n",
    "\n",
    "# all_data 만들기\n",
    "all_data = pd.concat([train, test], axis=0).copy()\n",
    "all_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "96f78210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- GridSearchCV를 사용한 하이퍼파라미터 튜닝 시작 ---\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "\n",
      "--- GridSearchCV 튜닝 결과 ---\n",
      "최적의 하이퍼파라미터: {'max_depth': 10, 'min_samples_leaf': 5}\n",
      "최적 파라미터일 때 CV R^2 평균 점수: 0.5455\n",
      "\n",
      "--- 과적합 해소 확인 ---\n",
      "  [기존 모델] 학습 R²: 0.7245 | CV R²: 0.2711 (Gap: 0.4534)\n",
      "  [튜닝된 모델] 학습 R²: 0.6249 | CV R²: 0.5455 (Gap: 0.0795)\n",
      "\n",
      "3. 추가 평가 지표:\n",
      "  - 평균 제곱 오차 (MSE): 77.84\n",
      "  - 평균 절대 오차 (MAE): 6.72\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# --- 이 부분은 이전 코드에서 이미 실행되었습니다 ---\n",
    "# rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "# X_train, y_train ...\n",
    "# -----------------------------------------------\n",
    "\n",
    "all_age = all_data[all_data['Age'].notnull()].copy()\n",
    "# print(all_age.describe())\n",
    "\n",
    "# 4. RandomForestRegressor 모델 학습\n",
    "# Age 예측 모델의 Feature(X)와 Target(Y)를 정의\n",
    "# Age 예측 모델에 사용할 Feature 리스트: Age와 연관성이 높고 수치화된 변수 사용\n",
    "age_features = ['Title_Encoded', 'Pclass', 'family_size', 'Sex_Encoded', 'Fare_log', 'Embarked_Encoded','is_alone']\n",
    "X_train = all_age[age_features]\n",
    "X_train = pd.concat([X_train, all_age[age_features]], axis=0)\n",
    "y_train = all_age['Age']\n",
    "y_train = pd.concat([y_train, all_age['Age']], axis=0)\n",
    "print(\"--- GridSearchCV를 사용한 하이퍼파라미터 튜닝 시작 ---\")\n",
    "\n",
    "# 1. 튜닝할 기본 모델 객체 생성\n",
    "# n_estimators=100은 유지하고, random_state=42로 고정합니다.\n",
    "rf_base = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# 2. 탐색할 하이퍼파라미터 그리드 정의\n",
    "#    'max_depth'와 'min_samples_leaf'의 후보 값들을 리스트로 지정합니다.\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 10],  # 트리의 최대 깊이 (규제)\n",
    "    'min_samples_leaf': [5, 10, 15, 20] # 리프 노드의 최소 샘플 수 (규제)\n",
    "}\n",
    "# 위 설정은 4 * 4 = 16개의 조합을 테스트합니다.\n",
    "# 각 조합마다 5-fold 교차 검증(cv=5)을 하므로, 총 16 * 5 = 80번의 학습이 수행됩니다.\n",
    "\n",
    "# 3. GridSearchCV 객체 생성\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_base,        # 1. 기본 모델\n",
    "    param_grid=param_grid,    # 2. 파라미터 그리드\n",
    "    cv=5,                     # 3. 교차 검증 폴드 수 (기존과 동일하게 5)\n",
    "    scoring='r2',             # 4. 평가 지표 (R-squared)\n",
    "    n_jobs=-1,                # 5. 사용 가능한 모든 CPU 코어 사용 (속도 향상)\n",
    "    verbose=2                 # 6. 검색 과정을 자세히 출력\n",
    ")\n",
    "\n",
    "# 4. 그리드 서치 실행 (데이터로 학습)\n",
    "#    X_train, y_train는 이전 코드에서 정의한 데이터입니다.\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 5. 튜닝 결과 확인\n",
    "print(\"\\n--- GridSearchCV 튜닝 결과 ---\")\n",
    "print(f\"최적의 하이퍼파라미터: {grid_search.best_params_}\")\n",
    "print(f\"최적 파라미터일 때 CV R^2 평균 점수: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# 6. 최적의 모델(best_estimator_)로 과적합이 얼마나 해소되었는지 확인\n",
    "#    grid_search.best_estimator_는 최적의 파라미터로 '전체 X_train' 데이터에\n",
    "#    대해 다시 학습된 모델입니다.\n",
    "best_rf_regressor = grid_search.best_estimator_\n",
    "\n",
    "# 최적 모델의 '학습 데이터'에 대한 성능 확인\n",
    "y_pred_best = best_rf_regressor.predict(X_train)\n",
    "r2_train_best = r2_score(y_train, y_pred_best)\n",
    "\n",
    "print(\"\\n--- 과적합 해소 확인 ---\")\n",
    "print(f\"  [기존 모델] 학습 R²: 0.7245 | CV R²: 0.2711 (Gap: {0.7245 - 0.2711:.4f})\")\n",
    "print(f\"  [튜닝된 모델] 학습 R²: {r2_train_best:.4f} | CV R²: {grid_search.best_score_:.4f} (Gap: {r2_train_best - grid_search.best_score_:.4f})\")\n",
    "\n",
    "# MSE 및 MAE 계산\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "mse = mean_squared_error(y_train, y_pred_best)\n",
    "mae = np.mean(np.abs(y_train - y_pred_best))\n",
    "\n",
    "print(\"\\n3. 추가 평가 지표:\")\n",
    "print(f\"  - 평균 제곱 오차 (MSE): {mse:.2f}\")\n",
    "print(f\"  - 평균 절대 오차 (MAE): {mae:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "9903c9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 3. 'Age' 결측치 예측 및 채우기 시작 ---\n",
      "263\n",
      "--- 4. 'Age' 결측치 채우기 완료 ---\n",
      "\n",
      "--- 최종 확인 ---\n",
      "all 데이터 'Age' 결측치 개수: 0\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# 5. 'Age' 결측치 예측 및 원본 데이터에 채우기\n",
    "# ======================================================================\n",
    "print(\"--- 3. 'Age' 결측치 예측 및 채우기 시작 ---\")\n",
    "\n",
    "# 5-1. 'Age'가 결측치인 데이터 추출\n",
    "all_null_age = all_data[all_data['Age'].isnull()].copy()\n",
    "#print(all_null_age.describe())\n",
    "\n",
    "# 5-4. 예측에 사용할 피처(X) 준비\n",
    "X_predict_train = all_null_age[age_features]\n",
    "#print(X_predict_train.describe())\n",
    "\n",
    "# 5-5. 최종 모델로 'Age' 예측\n",
    "predicted_age_train = best_rf_regressor.predict(X_predict_train) # (주의) 이 코드를 연속해서 실행 시 오류 -> clear all output & Run All\n",
    "print(predicted_age_train.size)\n",
    "\n",
    "# 5-6. (최종) 원본 DataFrame의 결측치에 예측된 'Age' 값 채우기\n",
    "# .loc[행 인덱서, 열 이름]을 사용하여 정확하게 값을 할당합니다.\n",
    "all_data.loc[all_data['Age'].isnull(), 'Age'] = predicted_age_train\n",
    "\n",
    "print(\"--- 4. 'Age' 결측치 채우기 완료 ---\")\n",
    "print(\"\\n--- 최종 확인 ---\")\n",
    "print(f\"all 데이터 'Age' 결측치 개수: {all_data['Age'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "5581edfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss percentage of missing values in train set\n",
      "PassengerId           0\n",
      "Survived              0\n",
      "Pclass                0\n",
      "Name                  0\n",
      "Sex                   0\n",
      "Age                   0\n",
      "SibSp                 0\n",
      "Parch                 0\n",
      "Ticket                0\n",
      "Fare                  0\n",
      "Cabin               687\n",
      "Embarked              0\n",
      "Title                 0\n",
      "Sex_Encoded           0\n",
      "Embarked_Encoded      0\n",
      "Title_Encoded         0\n",
      "family_size           0\n",
      "is_alone              0\n",
      "Fare_log              0\n",
      "dtype: int64\n",
      "\n",
      "loss percentage of missing values in test set\n",
      "PassengerId           0\n",
      "Pclass                0\n",
      "Name                  0\n",
      "Sex                   0\n",
      "Age                   0\n",
      "SibSp                 0\n",
      "Parch                 0\n",
      "Ticket                0\n",
      "Fare                  0\n",
      "Cabin               327\n",
      "Embarked              0\n",
      "Title                 0\n",
      "Sex_Encoded           0\n",
      "Embarked_Encoded      0\n",
      "Title_Encoded         0\n",
      "family_size           0\n",
      "is_alone              0\n",
      "Fare_log              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# train과 test에 age가 채워진 all_data 분배하기\n",
    "train = all_data[all_data['Survived'].notnull()].copy()\n",
    "test = all_data[all_data['Survived'].isnull()].copy()\n",
    "test.drop('Survived', axis=1, inplace=True)\n",
    "#print(test.describe())\n",
    "\n",
    "print(f\"loss percentage of missing values in train set\\n{train.isnull().sum()}\") # loss of missing values in train set\n",
    "print(f\"\\nloss percentage of missing values in test set\\n{test.isnull().sum()}\") # loss of missing values in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "ecde14d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 피처 엔지니어링 및 스케일링 완료 ---\n",
      "학습 데이터 형태: (891, 16)\n",
      "테스트 데이터 형태: (418, 16)\n",
      "\n",
      "--- 4. SVM 하이퍼파라미터 튜닝 시작 ---\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "\n",
      "--- 튜닝 결과 ---\n",
      "최적의 하이퍼파라미터: {'C': 100, 'gamma': 0.01}\n",
      "최적 파라미터일 때 CV 정확도 (평균): 0.8238\n",
      "\n",
      "--- 5. 예측 및 제출 파일 생성 ---\n",
      "\n",
      "--- 'svm_tuned_submission.csv' 파일 생성 완료 ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# --- 이 코드는 'train'과 'test' 데이터프레임이 이미 로드되어 있고, ---\n",
    "# --- 'Age' 결측치가 (이전 단계에서) 모두 채워졌다고 가정합니다. ---\n",
    "\n",
    "# --- 1. SVM을 위한 피처 엔지니어링 ---\n",
    "\n",
    "# Submission(제출)을 위해 test의 PassengerId를 저장해 둡니다.\n",
    "test_passenger_id = test['PassengerId']\n",
    "\n",
    "# y_train (정답)을 미리 분리합니다.\n",
    "y_train = train['Survived']\n",
    "\n",
    "# 나중에 원-핫 인코딩을 일관되게 적용하기 위해 train과 test를 잠시 합칩니다.\n",
    "# 'Survived' 열은 예측 대상이므로 합치기 전에 train에서 제거합니다.\n",
    "full_data = pd.concat([train.drop('Survived', axis=1), test], ignore_index=True)\n",
    "\n",
    "# --- 2. 피처 선택 및 원-핫 인코딩 ---\n",
    "\n",
    "# 모델에 사용할 피처 리스트\n",
    "# (주의: 'Age'는 사용하고, 'Fare' 대신 'Fare_log'를 사용)\n",
    "numerical_features = ['Age', 'family_size', 'Fare_log']\n",
    "categorical_features = ['Sex', 'Pclass', 'Embarked', 'Title', 'is_alone']\n",
    "\n",
    "# 불필요한 피처 제거\n",
    "drop_cols = ['PassengerId', 'Name', 'SibSp', 'Parch', 'Ticket', 'Cabin', 'Fare']\n",
    "full_data_processed = full_data.drop(columns=drop_cols)\n",
    "#print(full_data_processed.describe())\n",
    "\n",
    "\n",
    "# (중요) 범주형 피처에 대해 원-핫 인코딩 수행\n",
    "# pd.get_dummies는 문자열 피처를 자동으로 0과 1로 구성된 여러 열로 변환합니다.\n",
    "# drop_first=True는 다중공선성 문제를 피하기 위해 첫 번째 카테고리를 제거합니다.\n",
    "full_data_processed = pd.get_dummies(\n",
    "    full_data_processed, \n",
    "    columns=categorical_features, \n",
    "    drop_first=True\n",
    ")\n",
    "\n",
    "# --- 3. 데이터 분리 및 (필수) 스케일링 ---\n",
    "\n",
    "# 다시 train과 test로 분리합니다.\n",
    "X_train = full_data_processed.iloc[:len(train)]\n",
    "X_test = full_data_processed.iloc[len(train):]\n",
    "\n",
    "# (필수) StandardScaler로 SVM을 위한 스케일링 수행\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scaler는 *반드시* 'train' 데이터로만 'fit' 해야 합니다. (Data Leakage 방지)\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# 'train'과 'test' 데이터 모두 'transform'을 적용합니다.\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"--- 피처 엔지니어링 및 스케일링 완료 ---\")\n",
    "print(f\"학습 데이터 형태: {X_train_scaled.shape}\")\n",
    "print(f\"테스트 데이터 형태: {X_test_scaled.shape}\")\n",
    "\n",
    "#> **중요:** 스케일링은 SVM에서 선택이 아닌 **필수**입니다. 위 그림처럼, 스케일링을 하지 않으면 'Fare'같이 범위가 큰 피처가 결정 경계를 왜곡시켜 모델 성능이 크게 저하됩니다.\n",
    "print(\"\\n--- 4. SVM 하이퍼파라미터 튜닝 시작 ---\")\n",
    "\n",
    "# 4-1. 튜닝할 기본 모델 객체 생성\n",
    "svc_base = SVC(kernel='rbf', random_state=42)\n",
    "\n",
    "# 4-2. 탐색할 하이퍼파라미터 그리드 정의\n",
    "# (주의: 탐색 범위를 넓게 잡으면 튜닝 시간이 매우 오래 걸릴 수 있습니다)\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],            # 10의 거듭제곱으로 탐색\n",
    "    'gamma': ['scale', 0.1, 0.01]  # 'scale'(권장옵션) 및 10의 거듭제곱\n",
    "}\n",
    "\n",
    "# 4-3. GridSearchCV 객체 생성\n",
    "grid_search_svc = GridSearchCV(\n",
    "    estimator=svc_base,       # 기본 모델\n",
    "    param_grid=param_grid,    # 파라미터 그리드\n",
    "    cv=5,                     # 5-fold 교차 검증\n",
    "    scoring='accuracy',       # 평가 지표\n",
    "    n_jobs=-1,                # 모든 CPU 코어 사용\n",
    "    verbose=2                 # 튜닝 과정 출력\n",
    ")\n",
    "\n",
    "# 4-4. 튜닝 실행 (스케일링된 학습 데이터 사용)\n",
    "grid_search_svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 4-5. 튜닝 결과 확인\n",
    "print(\"\\n--- 튜닝 결과 ---\")\n",
    "print(f\"최적의 하이퍼파라미터: {grid_search_svc.best_params_}\")\n",
    "print(f\"최적 파라미터일 때 CV 정확도 (평균): {grid_search_svc.best_score_:.4f}\")\n",
    "\n",
    "# 4-6. 최적의 모델을 최종 모델로 사용\n",
    "# GridSearchCV는 'refit=True'가 기본값이므로,\n",
    "# 최적의 파라미터로 '전체 X_train_scaled' 데이터에 대해\n",
    "# 자동으로 다시 학습된 모델을 'best_estimator_'로 제공합니다.\n",
    "best_svc_model = grid_search_svc.best_estimator_\n",
    "\n",
    "# ======================================================================\n",
    "# --- 5. 예측 및 제출 파일 생성 --- (수정됨)\n",
    "# ======================================================================\n",
    "\n",
    "print(\"\\n--- 5. 예측 및 제출 파일 생성 ---\")\n",
    "\n",
    "# 5-1. (수정) 튜닝된 최적의 모델(best_svc_model)로 예측\n",
    "predictions = best_svc_model.predict(X_test_scaled)\n",
    "predictions_int = predictions.astype(int)\n",
    "\n",
    "# 5-2. 제출 파일 생성\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': test_passenger_id,\n",
    "    'Survived': predictions_int\n",
    "})\n",
    "\n",
    "# 5-3. CSV 파일로 저장 (이름 변경)\n",
    "submission.to_csv('svm_tuned_submission.csv', index=False)\n",
    "\n",
    "print(\"\\n--- 'svm_tuned_submission.csv' 파일 생성 완료 ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
